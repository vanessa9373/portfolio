# Lab K8S-02: Kubernetes Persistent Storage

![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?style=flat&logo=kubernetes&logoColor=white)
![Storage](https://img.shields.io/badge/Storage-PV%2FPVC-326CE5?style=flat)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&logo=docker&logoColor=white)
![YAML](https://img.shields.io/badge/YAML-CB171E?style=flat&logo=yaml&logoColor=white)

## Summary (The "Elevator Pitch")

Implemented the complete Kubernetes persistent storage lifecycle -- PersistentVolumes, PersistentVolumeClaims, StorageClasses, and dynamic provisioning -- to solve the problem of ephemeral container data. Data now survives pod restarts, rescheduling, and even node failures, with dynamic volume provisioning eliminating manual storage management.

## The Problem

Every time a pod restarted, **all data was lost**. Database files, user-uploaded content, application logs, session data -- everything stored inside the container's writable layer was ephemeral. The team was using `emptyDir` volumes as a workaround, but those only survived container restarts (not pod restarts). A database pod crashing at 3 AM meant total data loss and a restore from the last backup -- if one existed. **No one trusted running stateful workloads on Kubernetes** because the storage story was not implemented.

## The Solution

Implemented Kubernetes persistent storage from the ground up: **static PersistentVolumes** for pre-provisioned storage, **PersistentVolumeClaims** for developer self-service, **StorageClasses** for dynamic provisioning (no admin intervention needed), and **volume expansion** for growing data needs. The result: data persists across pod restarts, rescheduling, and upgrades. Developers request storage via PVCs without needing to know the underlying infrastructure, and volumes are provisioned automatically.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                     Kubernetes Cluster                           │
│                                                                  │
│  Developer Request          Storage Admin / Dynamic Provisioner  │
│  ┌────────────────┐         ┌───────────────────────────────┐   │
│  │ PVC             │         │ StorageClass                   │   │
│  │ "I need 10Gi    │──Bind──►│ provisioner: k8s.io/minikube  │   │
│  │  ReadWriteOnce" │         │ reclaimPolicy: Retain          │   │
│  └───────┬────────┘         │ allowVolumeExpansion: true     │   │
│          │                   └──────────────┬────────────────┘   │
│          │                                  │                    │
│          ▼                                  ▼ (auto-creates)    │
│  ┌────────────────┐         ┌───────────────────────────────┐   │
│  │ Pod              │         │ PersistentVolume (PV)          │   │
│  │ ┌──────────────┐│         │ capacity: 10Gi                 │   │
│  │ │ Container     ││         │ accessModes: ReadWriteOnce     │   │
│  │ │ mountPath:    ││◄──Mount─│ hostPath: /mnt/data            │   │
│  │ │ /data         ││         │ status: Bound                  │   │
│  │ └──────────────┘│         └───────────────────────────────┘   │
│  └────────────────┘                                              │
│                                                                  │
│  Volume Lifecycle:                                               │
│  Provisioning → Binding → Using → Reclaiming                    │
│  (Static/Dynamic)  (PVC↔PV)  (Pod mount)  (Retain/Delete)      │
│                                                                  │
│  Access Modes:                                                   │
│  RWO = ReadWriteOnce (single node)                              │
│  ROX = ReadOnlyMany  (many nodes, read-only)                    │
│  RWX = ReadWriteMany (many nodes, read-write)                   │
└─────────────────────────────────────────────────────────────────┘
```

## Tech Stack

| Technology | Purpose | Why I Chose It |
|------------|---------|----------------|
| Kubernetes | Container orchestration with storage primitives | Native PV/PVC/StorageClass support |
| PersistentVolume (PV) | Cluster-level storage resource | Decouples storage lifecycle from pod lifecycle |
| PersistentVolumeClaim (PVC) | Developer storage request | Self-service storage without infrastructure knowledge |
| StorageClass | Dynamic volume provisioning template | Eliminates manual PV creation; admin defines once, devs use many |
| hostPath | Local node storage (development only) | Simple for local dev/testing without cloud provider |
| kubectl | Kubernetes CLI | Primary tool for managing storage resources |
| Minikube/kind | Local Kubernetes cluster | Built-in storage provisioner for local development |

## Implementation Steps

### Step 1: Create a Static PersistentVolume (hostPath for Local Dev)
**What this does:** Creates a PersistentVolume manually -- this is "static provisioning" where an admin pre-creates storage. The hostPath type uses a directory on the node's filesystem. This is for dev only; production uses cloud block storage (EBS, Persistent Disk).
```bash
kubectl apply -f manifests/pv-static.yaml
kubectl get pv
kubectl describe pv static-pv-10gi
# Status should be "Available" (not yet bound to any PVC)
```

### Step 2: Create a PersistentVolumeClaim That Binds to It
**What this does:** Creates a PVC requesting 10Gi of ReadWriteOnce storage. Kubernetes matches this claim to the static PV based on capacity, access mode, and storageClassName. Once bound, no other PVC can use this PV.
```bash
kubectl apply -f manifests/pvc-static.yaml
kubectl get pvc -n k8s-storage
kubectl get pv
# PV status should change from "Available" to "Bound"
# PVC status should show "Bound" with the PV name
```

### Step 3: Deploy a Pod Mounting the PVC
**What this does:** Creates a pod that mounts the PVC at `/data`. The container writes a timestamp file to prove data is being persisted to the volume. Even if the pod is deleted, the data on the PV remains intact.
```bash
kubectl apply -f manifests/pod-with-pv.yaml
kubectl get pods -n k8s-storage
# Verify the mount
kubectl exec -it storage-demo -n k8s-storage -- ls -la /data
kubectl exec -it storage-demo -n k8s-storage -- cat /data/timestamp.txt
```

### Step 4: Create a StorageClass for Dynamic Provisioning
**What this does:** Defines a StorageClass that tells Kubernetes how to automatically create PVs when a PVC is submitted. The provisioner, reclaim policy, and volume expansion settings are all configured here. This is how production clusters work -- no manual PV creation needed.
```bash
kubectl apply -f manifests/storageclass.yaml
kubectl get storageclass
kubectl describe storageclass fast-storage
```

### Step 5: Deploy with Dynamic PVC (No Manual PV Needed)
**What this does:** Creates a PVC referencing the `fast-storage` StorageClass. Kubernetes automatically provisions a PV that matches the request -- no admin intervention. This is the preferred approach for production workloads.
```bash
kubectl apply -f manifests/pvc-dynamic.yaml
kubectl get pvc -n k8s-storage
kubectl get pv
# A new PV is automatically created and bound to the PVC
kubectl apply -f manifests/deployment-persistent.yaml
kubectl get pods -n k8s-storage
```

### Step 6: Test Data Persistence Across Pod Restarts
**What this does:** Writes data to the persistent volume, deletes the pod, waits for the Deployment to recreate it, and verifies the data survived. This proves that PVCs decouple data from the pod lifecycle.
```bash
# Write data to the volume
POD=$(kubectl get pod -l app=persistent-app -n k8s-storage -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $POD -n k8s-storage -- sh -c 'echo "Data written at $(date)" > /app/data/test-persistence.txt'
kubectl exec -it $POD -n k8s-storage -- cat /app/data/test-persistence.txt

# Delete the pod (Deployment recreates it)
kubectl delete pod $POD -n k8s-storage
kubectl get pods -n k8s-storage -w

# Verify data survived
NEW_POD=$(kubectl get pod -l app=persistent-app -n k8s-storage -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $NEW_POD -n k8s-storage -- cat /app/data/test-persistence.txt
# Output should show the same data from before the deletion
```

### Step 7: Expand Volume Size (Volume Expansion)
**What this does:** Increases the PVC from 5Gi to 10Gi. The StorageClass has `allowVolumeExpansion: true`, so Kubernetes resizes the underlying volume. Note: volume expansion is one-way -- you cannot shrink a PVC.
```bash
# Check current size
kubectl get pvc dynamic-pvc -n k8s-storage
# Patch the PVC to request more storage
kubectl patch pvc dynamic-pvc -n k8s-storage -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}'
# Watch the resize
kubectl get pvc dynamic-pvc -n k8s-storage -w
kubectl get pv
# Verify inside the pod
POD=$(kubectl get pod -l app=persistent-app -n k8s-storage -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $POD -n k8s-storage -- df -h /app/data
```

## Project Structure

```
K8S-02-persistent-storage/
├── README.md                          # Lab documentation (this file)
├── manifests/
│   ├── pv-static.yaml                 # Static PersistentVolume (hostPath)
│   ├── pvc-static.yaml                # PVC that binds to static PV
│   ├── storageclass.yaml              # StorageClass for dynamic provisioning
│   ├── pvc-dynamic.yaml               # PVC using dynamic StorageClass
│   ├── pod-with-pv.yaml               # Pod mounting static PVC
│   └── deployment-persistent.yaml     # Deployment with dynamic PVC
└── scripts/
    ├── deploy.sh                      # Apply all manifests in order
    ├── test-persistence.sh            # Write data, delete pod, verify survival
    └── cleanup.sh                     # Delete namespace and PVs
```

## Key Files Explained

| File | What It Does | Key Concepts |
|------|-------------|--------------|
| `manifests/pv-static.yaml` | Pre-provisions a 10Gi hostPath volume with Retain reclaim policy | Static provisioning, access modes, reclaim policies |
| `manifests/pvc-static.yaml` | Requests storage by capacity and access mode, binds to matching PV | Claim-based storage, binding rules, storageClassName matching |
| `manifests/storageclass.yaml` | Defines dynamic provisioning template with expansion support | Provisioner plugins, parameters, volume expansion |
| `manifests/pvc-dynamic.yaml` | Triggers automatic PV creation via StorageClass reference | Dynamic provisioning, no admin intervention |
| `manifests/pod-with-pv.yaml` | Mounts PVC at /data and writes timestamp to prove persistence | Volume mounts, init containers for data seeding |
| `manifests/deployment-persistent.yaml` | Deployment with PVC mount for stateful application data | Persistent workloads, data surviving pod restarts |
| `scripts/test-persistence.sh` | Automated test: write data, delete pod, verify data survives | Data lifecycle validation, automated testing |

## Results & Metrics

| Metric | Before (Ephemeral) | After (Persistent Storage) | Improvement |
|--------|---------------------|---------------------------|-------------|
| Data Loss on Restart | 100% (all data gone) | 0% (data survives) | **Zero data loss** |
| Storage Provisioning | Manual admin ticket | Dynamic (self-service) | **Minutes vs days** |
| Volume Resize | Recreate pod + volume | Online expansion | **Zero-downtime resize** |
| Stateful Workloads | "Don't run on K8s" | Fully supported | **Database-ready** |
| Storage Requests | Admin-managed | Developer self-service | **Eliminated bottleneck** |
| Recovery Time | Restore from backup | Instant (data on PV) | **Hours to seconds** |

## How I'd Explain This in an Interview

> "The team was afraid to run anything stateful on Kubernetes because every pod restart meant data loss. I implemented the full persistent storage stack: static PVs for legacy workloads, StorageClasses for dynamic provisioning so developers could self-service storage via PVCs without filing admin tickets, and volume expansion for growing data needs. The key insight is the three-layer abstraction -- StorageClass defines 'how' to provision, PV is the actual storage, and PVC is the developer's 'request'. This decouples developers from infrastructure: they just say 'I need 10Gi of fast storage' and Kubernetes handles the rest. After implementation, we went from zero stateful workloads on K8s to running PostgreSQL, Redis, and Elasticsearch with zero data loss incidents."

## Key Concepts Demonstrated

- **PersistentVolume (PV)** — A cluster-level storage resource with a lifecycle independent of any pod
- **PersistentVolumeClaim (PVC)** — A developer's request for storage; binds to a PV by capacity, access mode, and StorageClass
- **StorageClass** — A template that tells Kubernetes how to dynamically provision volumes using a specific provisioner
- **Dynamic Provisioning** — Automatic PV creation when a PVC references a StorageClass; eliminates manual admin work
- **Static Provisioning** — Admin pre-creates PVs that PVCs bind to; used for existing infrastructure or specialized storage
- **Access Modes** — RWO (single node read-write), ROX (multi-node read-only), RWX (multi-node read-write)
- **Reclaim Policies** — What happens to a PV when its PVC is deleted: Retain (keep data), Delete (destroy volume), Recycle (deprecated)
- **Volume Expansion** — Growing a PVC's requested storage after creation; requires StorageClass `allowVolumeExpansion: true`
- **hostPath** — Node-local storage for development; not suitable for production (data stuck on one node)
- **emptyDir** — Ephemeral volume that exists for the lifetime of a pod; useful for scratch space and inter-container sharing

## Lessons Learned

1. **Always use StorageClasses in production** — Static PV provisioning does not scale. StorageClasses let you define storage tiers (fast SSD, cheap HDD) once, and developers self-service via PVCs. This eliminated our storage admin bottleneck entirely.
2. **Set reclaimPolicy to Retain for databases** — The default `Delete` policy destroys the underlying volume when the PVC is deleted. One accidental `kubectl delete pvc` wiped a test database. Retain keeps the data safe and requires manual cleanup.
3. **hostPath is for development only** — It ties data to a specific node, so if the pod reschedules to another node, it loses access. In production, use cloud-provider volumes (EBS, Persistent Disk) or network-attached storage (NFS, Ceph).
4. **Test volume expansion before you need it** — Not all provisioners support expansion, and some require pod restarts. Verify expansion works in staging before a production emergency where the disk is 95% full.
5. **PVCs are namespace-scoped, PVs are not** — This catches people off guard. A PV is a cluster resource, but the PVC that binds to it is namespace-scoped. Deleting a namespace deletes the PVC, which may trigger reclaim on the PV depending on the policy.

## Author

**Jenella V.** — Solutions Architect & Cloud Engineer
- [LinkedIn](https://www.linkedin.com/in/jenella-v-4a4b963ab/) | [GitHub](https://github.com/vanessa9373) | [Portfolio](https://vanessa9373.github.io/portfolio/)
